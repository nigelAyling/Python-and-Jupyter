{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUR TASK. Import request & beutiful soup\n",
    "import requests\n",
    "import bs4\n",
    "\n",
    "def create_soup(url,tag_name):\n",
    "    page = requests.get(url)\n",
    "    soup = bs4.BeautifulSoup(page.text,\"lxml\")\n",
    "    result = soup.select(tag_name)\n",
    "    return result\n",
    "\n",
    "def get_text_from_tag(tag_list):\n",
    "    tag_number = len(tag_list)\n",
    "    result = []\n",
    "    for item in tag_list:\n",
    "        result.append(item.getText())\n",
    "    return result\n",
    "\n",
    "def get_99_urls(url_template):\n",
    "    result = []\n",
    "    for item in range(1,99):\n",
    "        result.append(url_template + str(item))\n",
    "    return result\n",
    "    \n",
    "def page_scrape_multiple_pages(url,tag_name):\n",
    "    #Get 99 Urls\n",
    "    imput_urls = get_99_urls(url)\n",
    "    page_has_content = True\n",
    "    master_result_list = []\n",
    "     \n",
    "    while page_has_content == True:\n",
    "        for page_number in range(1,200):\n",
    "            current_page = create_soup(imput_urls[page_number-1],tag_name)\n",
    "            current_page_author_list = get_text_from_tag(current_page)\n",
    "\n",
    "            if len(current_page_author_list) == 0:\n",
    "                print('List is empty')\n",
    "                page_has_content = False\n",
    "                break  \n",
    "\n",
    "            #print('Page Number',page_number)\n",
    "            #print('Page Text:',current_page_author_list,'\\n')\n",
    "            # this is where we construct our master list of all records\n",
    "            master_result_list = master_result_list + current_page_author_list\n",
    "            #print('Page Text:',master_result_list,'\\n')\n",
    "            \n",
    "    for item in master_result_list:\n",
    "        print(item)\n",
    "        \n",
    "    return master_result_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://github.com/CSSEGISandData/COVID-19/blob/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'\n",
    "tag = '.js-file-line'\n",
    "page = requests.get(url)\n",
    "soup = bs4.BeautifulSoup(page.text,\"lxml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Country/Region': 'Afghanistan', 'Lat': '33.93911'}, {'Country/Region': 'Albania', 'Lat': '41.1533'}, {'Country/Region': 'Algeria', 'Lat': '28.0339'}, {'Country/Region': 'Andorra', 'Lat': '42.5063'}]\n"
     ]
    }
   ],
   "source": [
    "#Run Scraper To get data\n",
    "url = 'https://github.com/CSSEGISandData/COVID-19/blob/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'\n",
    "tag = '.js-file-line'\n",
    "data = create_soup(url,tag)\n",
    "#Headers only\n",
    "headers = data[0].getText()\n",
    "headers_list = headers.split('\\n')\n",
    "\n",
    "#print(headers_list[2:9])\n",
    "#second Row\n",
    "row1 = data[1].getText()\n",
    "row1_list = row1.split('\\n')\n",
    "#print(row1_list[2:9])\n",
    "\n",
    "row1_list_sample = row1_list[2:9]\n",
    "header_list_sample = headers_list[2:9]\n",
    "\n",
    "#Now use these two lists and make a dictionary from them\n",
    "\n",
    "dictionary = dict(zip(header_list_sample, row1_list_sample))\n",
    "\n",
    "\n",
    "# Lets do this for the entire data\n",
    "#number_of_rows = len(data)-1\n",
    "#print(number_of_rows)\n",
    "\n",
    "#Create Resulting List\n",
    "summary_list = []\n",
    "#Create Header Row\n",
    "headers = data[0].getText()\n",
    "headers_list = headers.split('\\n')\n",
    "headers_list_cleaned = headers_list[3:5]\n",
    "\n",
    "\n",
    "for item in data[1:5]:\n",
    "    item_text = item.getText()\n",
    "    item_list = item_text.split('\\n')\n",
    "    item_list_cleaned = item_list[3:5]\n",
    "    dictionary_row = dict(zip(headers_list_cleaned, item_list_cleaned))\n",
    "    #print(dictionary_row)\n",
    "    # put it into summary list\n",
    "    summary_list.append(dictionary_row)\n",
    "    \n",
    "print(summary_list)\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
